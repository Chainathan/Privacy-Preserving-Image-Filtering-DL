{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44df94c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Sai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import wandb\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e67f29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering dataset for English samples only...\n",
      "Train size: 34800\n",
      "Validation size: 8701\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"ai4privacy/pii-masking-200k\")\n",
    "\n",
    "# Filter to keep only English samples\n",
    "print(\"Filtering dataset for English samples only...\")\n",
    "dataset_en = dataset.filter(lambda example: example[\"language\"] == \"en\")\n",
    "\n",
    "# Split the train dataset into train and validation\n",
    "train_test_split = dataset_en[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Create a new DatasetDict with the splits\n",
    "dataset = datasets.DatasetDict({\n",
    "    \"train\": train_test_split[\"train\"],\n",
    "    \"validation\": train_test_split[\"test\"]  # The \"test\" split from train_test_split becomes our validation\n",
    "})\n",
    "\n",
    "print(f\"Train size: {len(dataset['train'])}\")\n",
    "print(f\"Validation size: {len(dataset['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44eb36a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_text: This is to notify you about the changes in your tax plan. The state of Basel-Landschaft and county of Lake County have revised laws affecting businesses. Will connect with Legacy Branding Associate soon. Check in with https://basic-reparation.com/ for details.\n",
      "target_text: This is to notify you about the changes in your tax plan. The state of [STATE] and county of [COUNTY] have revised laws affecting businesses. Will connect with [JOBTITLE] soon. Check in with [URL] for details.\n",
      "privacy_mask: [{'value': 'Basel-Landschaft', 'start': 71, 'end': 87, 'label': 'STATE'}, {'value': 'Lake County', 'start': 102, 'end': 113, 'label': 'COUNTY'}, {'value': 'Legacy Branding Associate', 'start': 172, 'end': 197, 'label': 'JOBTITLE'}, {'value': 'https://basic-reparation.com/', 'start': 218, 'end': 247, 'label': 'URL'}]\n",
      "span_labels: [[0, 71, \"O\"], [71, 87, \"STATE\"], [87, 102, \"O\"], [102, 113, \"COUNTY\"], [113, 172, \"O\"], [172, 197, \"JOBTITLE\"], [197, 218, \"O\"], [218, 247, \"URL\"], [247, 260, \"O\"]]\n",
      "mbert_text_tokens: ['This', 'is', 'to', 'noti', '##fy', 'you', 'about', 'the', 'changes', 'in', 'your', 'tax', 'plan', '.', 'The', 'state', 'of', 'Basel', '-', 'Landschaft', 'and', 'county', 'of', 'Lake', 'County', 'have', 'revised', 'laws', 'affect', '##ing', 'businesses', '.', 'Will', 'connect', 'with', 'Legacy', 'Brand', '##ing', 'Associate', 'soon', '.', 'Check', 'in', 'with', 'https', ':', '/', '/', 'basic', '-', 'rep', '##aration', '.', 'com', '/', 'for', 'details', '.']\n",
      "mbert_bio_labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STATE', 'I-STATE', 'I-STATE', 'O', 'O', 'O', 'B-COUNTY', 'I-COUNTY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-JOBTITLE', 'I-JOBTITLE', 'I-JOBTITLE', 'I-JOBTITLE', 'O', 'O', 'O', 'O', 'O', 'B-URL', 'I-URL', 'I-URL', 'I-URL', 'I-URL', 'I-URL', 'I-URL', 'I-URL', 'I-URL', 'I-URL', 'I-URL', 'O', 'O', 'O']\n",
      "id: 181469\n",
      "language: en\n",
      "set: train\n",
      "Number of samples: 34800\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[\"train\"][0]\n",
    "for key, value in sample.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Number of samples: {len(dataset['train'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ea27c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source_text': Value(dtype='string', id=None),\n",
       " 'target_text': Value(dtype='string', id=None),\n",
       " 'privacy_mask': [{'value': Value(dtype='string', id=None),\n",
       "   'start': Value(dtype='int64', id=None),\n",
       "   'end': Value(dtype='int64', id=None),\n",
       "   'label': Value(dtype='string', id=None)}],\n",
       " 'span_labels': Value(dtype='string', id=None),\n",
       " 'mbert_text_tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'mbert_bio_labels': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'id': Value(dtype='int64', id=None),\n",
       " 'language': Value(dtype='string', id=None),\n",
       " 'set': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67168894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting unique labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34800/34800 [00:07<00:00, 4933.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 113\n",
      "Labels: ['O', 'B-ACCOUNTNAME', 'I-ACCOUNTNAME', 'B-ACCOUNTNUMBER', 'I-ACCOUNTNUMBER', 'B-AGE', 'I-AGE', 'B-AMOUNT', 'I-AMOUNT', 'B-BIC'] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create label mappings from the dataset\n",
    "# First, extract all unique labels from the privacy_mask field\n",
    "print(\"Extracting unique labels...\")\n",
    "entity_labels = set()\n",
    "for example in tqdm(dataset[\"train\"]):\n",
    "    for mask in example[\"privacy_mask\"]:\n",
    "        entity_labels.add(mask[\"label\"])\n",
    "\n",
    "# Create BIO tags for each entity type\n",
    "label_list = [\"O\"]  # Outside tag\n",
    "for entity in sorted(entity_labels):\n",
    "    label_list.append(f\"B-{entity}\")  # Beginning tag\n",
    "    label_list.append(f\"I-{entity}\")  # Inside tag\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "\n",
    "print(f\"Number of labels: {len(label_list)}\")\n",
    "print(\"Labels:\", label_list[:10], \"...\" if len(label_list) > 10 else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "221ef6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load MobileBERT tokenizer and model\n",
    "model_name = \"google/mobilebert-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\n",
    "#     model_name,\n",
    "#     num_labels=len(label_list),\n",
    "#     id2label=id2label,\n",
    "#     label2id=label2id\n",
    "# )\n",
    "# model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d30b5feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process dataset with MobileBERT tokenizer\n",
    "def prepare_with_mobilebert_tokenizer(examples, tokenizer=tokenizer, max_length=256, label2id=label2id):\n",
    "    \"\"\"\n",
    "    Tokenize text with MobileBERT's tokenizer and align labels based on character offsets.\n",
    "    \n",
    "    This function:\n",
    "    1. Takes the raw text from source_text\n",
    "    2. Uses the privacy_mask information to build a character-level entity map\n",
    "    3. Tokenizes with MobileBERT's tokenizer and gets character offsets\n",
    "    4. Aligns entity labels with the new tokens\n",
    "    \"\"\"\n",
    "    # Get the original text\n",
    "    texts = examples[\"source_text\"]\n",
    "    \n",
    "    # Extract character-level entity information from privacy_mask\n",
    "    entity_maps = []\n",
    "    for example_masks in examples[\"privacy_mask\"]:\n",
    "        entity_map = {}\n",
    "        for mask in example_masks:\n",
    "            for pos in range(mask[\"start\"], mask[\"end\"]):\n",
    "                entity_map[pos] = mask[\"label\"]\n",
    "        entity_maps.append(entity_map)\n",
    "    \n",
    "    # Tokenize with MobileBERT's tokenizer and get character offsets\n",
    "    tokenized = tokenizer(\n",
    "        texts, \n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=False,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    \n",
    "    # Align labels with new tokens\n",
    "    labels = []\n",
    "    for i, offset_mapping in enumerate(tokenized.pop(\"offset_mapping\")):\n",
    "        label_ids = []\n",
    "        entity_map = entity_maps[i]\n",
    "        \n",
    "        previous_entity = None\n",
    "        for j, (start, end) in enumerate(offset_mapping):\n",
    "            # Skip special tokens which have empty offsets (0,0)\n",
    "            if start == end == 0:\n",
    "                label_ids.append(-100)\n",
    "                continue\n",
    "                \n",
    "            # Find if this token overlaps with any entity\n",
    "            current_entity = None\n",
    "            for pos in range(start, end):\n",
    "                if pos in entity_map:\n",
    "                    current_entity = entity_map[pos]\n",
    "                    break\n",
    "            \n",
    "            # Determine if this is a beginning or inside token\n",
    "            if current_entity is None:\n",
    "                # Not an entity\n",
    "                label_ids.append(label2id[\"O\"])\n",
    "                previous_entity = None\n",
    "            elif previous_entity != current_entity:\n",
    "                # Beginning of entity or new entity\n",
    "                label_ids.append(label2id[f\"B-{current_entity}\"])\n",
    "                previous_entity = current_entity\n",
    "            else:\n",
    "                # Continuation of the entity\n",
    "                label_ids.append(label2id[f\"I-{current_entity}\"])\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    prepare_with_mobilebert_tokenizer,\n",
    "    batched=True,\n",
    "    batch_size=32,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50e7de1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [101, 2023, 2003, 2000, 2025, 8757, 2017, 2055, 1996, 3431, 1999, 2115, 4171, 2933, 1012, 1996, 2110, 1997, 14040, 1011, 4915, 29043, 1998, 2221, 1997, 2697, 2221, 2031, 8001, 4277, 12473, 5661, 1012, 2097, 7532, 2007, 8027, 16140, 5482, 2574, 1012, 4638, 1999, 2007, 16770, 1024, 1013, 1013, 3937, 1011, 16360, 25879, 3258, 1012, 4012, 1013, 2005, 4751, 1012, 102]\n",
      "token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "labels: [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 95, 96, 96, 96, 0, 0, 0, 19, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 61, 62, 62, 0, 0, 0, 0, 0, 101, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 0, 0, 0, -100]\n",
      "Number of samples: 34800\n"
     ]
    }
   ],
   "source": [
    "sample = tokenized_datasets[\"train\"][0]\n",
    "for key, value in sample.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f\"Number of samples: {len(dataset['train'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "019f5fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing sanity check on 5 samples...\n",
      "\n",
      "--- Sample 1 ---\n",
      "Token\tLabel\n",
      "------------------------------\n",
      "[CLS]\tSPECIAL\n",
      "this\tO\n",
      "is\tO\n",
      "to\tO\n",
      "not\tO\n",
      "##ify\tO\n",
      "you\tO\n",
      "about\tO\n",
      "the\tO\n",
      "changes\tO\n",
      "in\tO\n",
      "your\tO\n",
      "tax\tO\n",
      "plan\tO\n",
      ".\tO\n",
      "the\tO\n",
      "state\tO\n",
      "of\tO\n",
      "basel\tB-STATE\n",
      "-\tI-STATE\n",
      "lands\tI-STATE\n",
      "##chaft\tI-STATE\n",
      "and\tO\n",
      "county\tO\n",
      "of\tO\n",
      "lake\tB-COUNTY\n",
      "county\tI-COUNTY\n",
      "have\tO\n",
      "revised\tO\n",
      "laws\tO\n",
      "affecting\tO\n",
      "businesses\tO\n",
      ".\tO\n",
      "will\tO\n",
      "connect\tO\n",
      "with\tO\n",
      "legacy\tB-JOBTITLE\n",
      "branding\tI-JOBTITLE\n",
      "associate\tI-JOBTITLE\n",
      "soon\tO\n",
      ".\tO\n",
      "check\tO\n",
      "in\tO\n",
      "with\tO\n",
      "https\tB-URL\n",
      ":\tI-URL\n",
      "/\tI-URL\n",
      "/\tI-URL\n",
      "basic\tI-URL\n",
      "-\tI-URL\n",
      "rep\tI-URL\n",
      "##arat\tI-URL\n",
      "##ion\tI-URL\n",
      ".\tI-URL\n",
      "com\tI-URL\n",
      "/\tI-URL\n",
      "for\tO\n",
      "details\tO\n",
      ".\tO\n",
      "[SEP]\tSPECIAL\n",
      "\n",
      "Label distribution:\n",
      "O: 37\n",
      "B-STATE: 1\n",
      "I-STATE: 3\n",
      "B-COUNTY: 1\n",
      "I-COUNTY: 1\n",
      "B-JOBTITLE: 1\n",
      "I-JOBTITLE: 2\n",
      "B-URL: 1\n",
      "I-URL: 11\n",
      "\n",
      "--- Sample 2 ---\n",
      "Token\tLabel\n",
      "------------------------------\n",
      "[CLS]\tSPECIAL\n",
      "dear\tO\n",
      "drew\tB-FIRSTNAME\n",
      ",\tO\n",
      "your\tO\n",
      "mentor\tO\n",
      "##ship\tO\n",
      "program\tO\n",
      "application\tO\n",
      "is\tO\n",
      "tentatively\tO\n",
      "approved\tO\n",
      "!\tO\n",
      "please\tO\n",
      "confirm\tO\n",
      "if\tO\n",
      "all\tO\n",
      "details\tO\n",
      "including\tO\n",
      "04\tB-DOB\n",
      "/\tI-DOB\n",
      "02\tI-DOB\n",
      "/\tI-DOB\n",
      "1974\tI-DOB\n",
      "and\tO\n",
      "81\tB-SSN\n",
      "##6\tI-SSN\n",
      "75\tI-SSN\n",
      "52\tI-SSN\n",
      "##32\tI-SSN\n",
      "are\tO\n",
      "correct\tO\n",
      ".\tO\n",
      "[SEP]\tSPECIAL\n",
      "\n",
      "Label distribution:\n",
      "O: 21\n",
      "B-FIRSTNAME: 1\n",
      "B-DOB: 1\n",
      "I-DOB: 4\n",
      "B-SSN: 1\n",
      "I-SSN: 4\n",
      "\n",
      "--- Sample 3 ---\n",
      "Token\tLabel\n",
      "------------------------------\n",
      "[CLS]\tSPECIAL\n",
      "the\tO\n",
      "patient\tO\n",
      "at\tO\n",
      "333\tB-BUILDINGNUMBER\n",
      "has\tO\n",
      "been\tO\n",
      "asking\tO\n",
      "for\tO\n",
      "a\tO\n",
      "device\tO\n",
      "with\tO\n",
      "mac\tO\n",
      "address\tO\n",
      "ee\tB-MAC\n",
      ":\tI-MAC\n",
      "fa\tI-MAC\n",
      ":\tI-MAC\n",
      "e\tI-MAC\n",
      "##5\tI-MAC\n",
      ":\tI-MAC\n",
      "08\tI-MAC\n",
      ":\tI-MAC\n",
      "db\tI-MAC\n",
      ":\tI-MAC\n",
      "a\tI-MAC\n",
      "##0\tI-MAC\n",
      ".\tO\n",
      "anyone\tO\n",
      "knows\tO\n",
      "what\tO\n",
      "this\tO\n",
      "is\tO\n",
      "about\tO\n",
      "?\tO\n",
      "[SEP]\tSPECIAL\n",
      "\n",
      "Label distribution:\n",
      "O: 20\n",
      "B-BUILDINGNUMBER: 1\n",
      "B-MAC: 1\n",
      "I-MAC: 12\n",
      "\n",
      "--- Sample 4 ---\n",
      "Token\tLabel\n",
      "------------------------------\n",
      "[CLS]\tSPECIAL\n",
      "dear\tO\n",
      "miss\tB-PREFIX\n",
      ",\tO\n",
      "your\tO\n",
      "child\tO\n",
      "'\tO\n",
      "s\tO\n",
      "health\tO\n",
      "is\tO\n",
      "our\tO\n",
      "top\tO\n",
      "priority\tO\n",
      ".\tO\n",
      "we\tO\n",
      "understand\tO\n",
      "payment\tO\n",
      "complications\tO\n",
      ",\tO\n",
      "hence\tO\n",
      "we\tO\n",
      "are\tO\n",
      "looking\tO\n",
      "into\tO\n",
      "your\tO\n",
      "billing\tO\n",
      "query\tO\n",
      "on\tO\n",
      "your\tO\n",
      "guyana\tB-CURRENCY\n",
      "dollar\tI-CURRENCY\n",
      "in\tO\n",
      "##vo\tO\n",
      "##ice\tO\n",
      "of\tO\n",
      "93\tB-AMOUNT\n",
      "##7\tI-AMOUNT\n",
      "##6\tI-AMOUNT\n",
      "##2\tI-AMOUNT\n",
      ".\tI-AMOUNT\n",
      "24\tI-AMOUNT\n",
      ".\tO\n",
      "this\tO\n",
      "is\tO\n",
      "referenced\tO\n",
      "to\tO\n",
      "your\tO\n",
      "account\tO\n",
      "number\tO\n",
      "240\tB-ACCOUNTNUMBER\n",
      "##52\tI-ACCOUNTNUMBER\n",
      "##17\tI-ACCOUNTNUMBER\n",
      "##2\tI-ACCOUNTNUMBER\n",
      "of\tO\n",
      "metz\tB-COMPANYNAME\n",
      "llc\tI-COMPANYNAME\n",
      ".\tO\n",
      "kindly\tO\n",
      "be\tO\n",
      "patient\tO\n",
      "as\tO\n",
      "we\tO\n",
      "resolve\tO\n",
      "this\tO\n",
      ".\tO\n",
      "[SEP]\tSPECIAL\n",
      "\n",
      "Label distribution:\n",
      "O: 49\n",
      "B-PREFIX: 1\n",
      "B-CURRENCY: 1\n",
      "I-CURRENCY: 1\n",
      "B-AMOUNT: 1\n",
      "I-AMOUNT: 5\n",
      "B-ACCOUNTNUMBER: 1\n",
      "I-ACCOUNTNUMBER: 3\n",
      "B-COMPANYNAME: 1\n",
      "I-COMPANYNAME: 1\n",
      "\n",
      "--- Sample 5 ---\n",
      "Token\tLabel\n",
      "------------------------------\n",
      "[CLS]\tSPECIAL\n",
      "we\tO\n",
      "'\tO\n",
      "re\tO\n",
      "looking\tO\n",
      "at\tO\n",
      "potential\tO\n",
      "liability\tO\n",
      "issues\tO\n",
      "with\tO\n",
      "property\tO\n",
      "94\tB-BUILDINGNUMBER\n",
      "##43\tI-BUILDINGNUMBER\n",
      "prospect\tB-STREET\n",
      "road\tI-STREET\n",
      "due\tO\n",
      "to\tO\n",
      "a\tO\n",
      "faulty\tO\n",
      "deck\tO\n",
      "installation\tO\n",
      ".\tO\n",
      "[SEP]\tSPECIAL\n",
      "\n",
      "Label distribution:\n",
      "O: 17\n",
      "B-BUILDINGNUMBER: 1\n",
      "I-BUILDINGNUMBER: 1\n",
      "B-STREET: 1\n",
      "I-STREET: 1\n",
      "\n",
      "--- Overall Dataset Statistics ---\n",
      "Number of training examples: 34800\n",
      "Number of validation examples: 8701\n",
      "Average token length (train): 48.18\n",
      "Max token length (train): 256\n",
      "Min token length (train): 11\n",
      "\n",
      "Top 10 most common labels in training set:\n",
      "O: 901719 (56.11%)\n",
      "I-USERAGENT: 71955 (4.48%)\n",
      "I-IPV6: 56170 (3.50%)\n",
      "I-BITCOINADDRESS: 42734 (2.66%)\n",
      "I-ETHEREUMADDRESS: 38112 (2.37%)\n",
      "I-IP: 28984 (1.80%)\n",
      "I-EMAIL: 27096 (1.69%)\n",
      "I-IBAN: 22501 (1.40%)\n",
      "I-URL: 22438 (1.40%)\n",
      "I-CREDITCARDNUMBER: 17369 (1.08%)\n",
      "\n",
      "Least common labels in training set:\n",
      "B-LITECOINADDRESS: 656 (0.04%)\n",
      "B-CURRENCYCODE: 650 (0.04%)\n",
      "B-BIC: 645 (0.04%)\n",
      "B-CURRENCYNAME: 638 (0.04%)\n",
      "B-PIN: 611 (0.04%)\n",
      "I-CURRENCYCODE: 599 (0.04%)\n",
      "I-JOBTYPE: 544 (0.03%)\n",
      "I-CREDITCARDCVV: 452 (0.03%)\n",
      "I-JOBAREA: 232 (0.01%)\n",
      "I-MIDDLENAME: 158 (0.01%)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check for tokenized dataset\n",
    "def sanity_check_tokenized_dataset(tokenized_dataset, tokenizer, id2label, num_samples=5):\n",
    "    \"\"\"\n",
    "    Perform sanity checks on the tokenized dataset to ensure proper alignment \n",
    "    between tokens and labels.\n",
    "    \n",
    "    Args:\n",
    "        tokenized_dataset: The dataset after tokenization and processing\n",
    "        tokenizer: The tokenizer used to process the dataset\n",
    "        id2label: Mapping from label IDs to label names\n",
    "        num_samples: Number of samples to check\n",
    "    \"\"\"\n",
    "    print(f\"Performing sanity check on {num_samples} samples...\")\n",
    "    \n",
    "    # Get a subset of samples\n",
    "    samples = tokenized_dataset[\"train\"].select(range(min(num_samples, len(tokenized_dataset[\"train\"]))))\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        \n",
    "        # Get input IDs and labels\n",
    "        input_ids = sample[\"input_ids\"]\n",
    "        labels = sample[\"labels\"]\n",
    "        \n",
    "        # Decode tokens\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        \n",
    "        # Print alignment of tokens and labels\n",
    "        print(\"Token\\tLabel\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        for token, label_id in zip(tokens, labels):\n",
    "            # Skip tokens with label -100 (special tokens)\n",
    "            if label_id == -100:\n",
    "                label_text = \"SPECIAL\"\n",
    "            else:\n",
    "                label_text = id2label[label_id]\n",
    "                \n",
    "            print(f\"{token}\\t{label_text}\")\n",
    "        \n",
    "        # Check label distribution\n",
    "        label_counts = {}\n",
    "        for label_id in labels:\n",
    "            if label_id != -100:\n",
    "                label_name = id2label[label_id]\n",
    "                label_counts[label_name] = label_counts.get(label_name, 0) + 1\n",
    "        \n",
    "        print(\"\\nLabel distribution:\")\n",
    "        for label, count in label_counts.items():\n",
    "            print(f\"{label}: {count}\")\n",
    "        \n",
    "        # Check if special tokens have -100 labels\n",
    "        special_token_ids = [tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id]\n",
    "        for idx, input_id in enumerate(input_ids):\n",
    "            if input_id in special_token_ids and labels[idx] != -100:\n",
    "                print(f\"WARNING: Special token {tokenizer.convert_ids_to_tokens([input_id])[0]} has label {id2label[labels[idx]]} instead of -100\")\n",
    "    \n",
    "    # Check overall dataset statistics\n",
    "    print(\"\\n--- Overall Dataset Statistics ---\")\n",
    "    print(f\"Number of training examples: {len(tokenized_dataset['train'])}\")\n",
    "    print(f\"Number of validation examples: {len(tokenized_dataset['validation'])}\")\n",
    "    \n",
    "    # Check token length distribution\n",
    "    train_lengths = [len(x[\"input_ids\"]) for x in tokenized_dataset[\"train\"]]\n",
    "    print(f\"Average token length (train): {sum(train_lengths) / len(train_lengths):.2f}\")\n",
    "    print(f\"Max token length (train): {max(train_lengths)}\")\n",
    "    print(f\"Min token length (train): {min(train_lengths)}\")\n",
    "    \n",
    "    # Check label distribution in training set\n",
    "    all_labels = []\n",
    "    for example in tokenized_dataset[\"train\"]:\n",
    "        for label in example[\"labels\"]:\n",
    "            if label != -100:\n",
    "                all_labels.append(label)\n",
    "    \n",
    "    label_count = {}\n",
    "    for label in all_labels:\n",
    "        label_name = id2label[label]\n",
    "        label_count[label_name] = label_count.get(label_name, 0) + 1\n",
    "    \n",
    "    print(\"\\nTop 10 most common labels in training set:\")\n",
    "    sorted_labels = sorted(label_count.items(), key=lambda x: x[1], reverse=True)\n",
    "    for label, count in sorted_labels[:10]:\n",
    "        print(f\"{label}: {count} ({count / len(all_labels) * 100:.2f}%)\")\n",
    "    \n",
    "    print(\"\\nLeast common labels in training set:\")\n",
    "    for label, count in sorted_labels[-10:]:\n",
    "        print(f\"{label}: {count} ({count / len(all_labels) * 100:.2f}%)\")\n",
    "\n",
    "\n",
    "# Perform sanity check on the tokenized dataset\n",
    "sanity_check_tokenized_dataset(tokenized_datasets, tokenizer, id2label, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2568450e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileBertForTokenClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def convert_to_binary_classification(examples):\n",
    "    labels = examples[\"labels\"]\n",
    "    binary_labels = []\n",
    "    \n",
    "    for example_labels in labels:\n",
    "        example_binary_labels = []\n",
    "        for label in example_labels:\n",
    "            if label == -100:  # Keep special token labels intact\n",
    "                example_binary_labels.append(-100)\n",
    "            elif label == label2id[\"O\"]:  # Keep \"O\" label intact\n",
    "                example_binary_labels.append(0)  # 0 for non-PII\n",
    "            else:  # All PII types become a single class\n",
    "                example_binary_labels.append(1)  # 1 for PII\n",
    "                \n",
    "        binary_labels.append(example_binary_labels)\n",
    "    \n",
    "    examples[\"labels\"] = binary_labels\n",
    "    return examples\n",
    "\n",
    "# Create a binary version of the dataset\n",
    "binary_id2label = {0: \"O\", 1: \"PII\"}\n",
    "binary_label2id = {\"O\": 0, \"PII\": 1}\n",
    "\n",
    "# Apply the conversion\n",
    "binary_tokenized_datasets = tokenized_datasets.map(\n",
    "    convert_to_binary_classification,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Update model configuration for binary classification\n",
    "binary_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    id2label=binary_id2label,\n",
    "    label2id=binary_label2id\n",
    ")\n",
    "binary_model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c0ac2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [101, 2023, 2003, 2000, 2025, 8757, 2017, 2055, 1996, 3431, 1999, 2115, 4171, 2933, 1012, 1996, 2110, 1997, 14040, 1011, 4915, 29043, 1998, 2221, 1997, 2697, 2221, 2031, 8001, 4277, 12473, 5661, 1012, 2097, 7532, 2007, 8027, 16140, 5482, 2574, 1012, 4638, 1999, 2007, 16770, 1024, 1013, 1013, 3937, 1011, 16360, 25879, 3258, 1012, 4012, 1013, 2005, 4751, 1012, 102]\n",
      "token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "labels: [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, -100]\n",
      "Source text: This is to notify you about the changes in your tax plan. The state of Basel-Landschaft and county of Lake County have revised laws affecting businesses. Will connect with Legacy Branding Associate soon. Check in with https://basic-reparation.com/ for details.\n",
      "Untokenized: ['[CLS]', 'this', 'is', 'to', 'not', '##ify', 'you', 'about', 'the', 'changes', 'in', 'your', 'tax', 'plan', '.', 'the', 'state', 'of', 'basel', '-', 'lands', '##chaft', 'and', 'county', 'of', 'lake', 'county', 'have', 'revised', 'laws', 'affecting', 'businesses', '.', 'will', 'connect', 'with', 'legacy', 'branding', 'associate', 'soon', '.', 'check', 'in', 'with', 'https', ':', '/', '/', 'basic', '-', 'rep', '##arat', '##ion', '.', 'com', '/', 'for', 'details', '.', '[SEP]']\n",
      "Number of samples: 34800\n"
     ]
    }
   ],
   "source": [
    "sample = binary_tokenized_datasets[\"train\"][0]\n",
    "sample1 = dataset[\"train\"][0]\n",
    "for key, value in sample.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(f'Source text: {sample1[\"source_text\"]}')\n",
    "untokenized = tokenizer.convert_ids_to_tokens(sample[\"input_ids\"])\n",
    "print(f\"Untokenized: {untokenized}\")\n",
    "print(f\"Number of samples: {len(dataset['train'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33eca50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "# def compute_metrics(p):\n",
    "#     predictions, labels = p\n",
    "#     predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "#     true_predictions = [\n",
    "#         [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "#         for prediction, label in zip(predictions, labels)\n",
    "#     ]\n",
    "#     true_labels = [\n",
    "#         [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "#         for prediction, label in zip(predictions, labels)\n",
    "#     ]\n",
    "\n",
    "#     results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "#     return {\n",
    "#         \"precision\": results[\"overall_precision\"],\n",
    "#         \"recall\": results[\"overall_recall\"],\n",
    "#         \"f1\": results[\"overall_f1\"],\n",
    "#         \"accuracy\": results[\"overall_accuracy\"],\n",
    "#     }\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_predictions = [\n",
    "        [binary_id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [binary_id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64634216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchainathanss\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "wandb.login()\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"pii_NER\"  # Name of your W&B project\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"       # Options: 'checkpoint', 'end', or 'false'\n",
    "os.environ[\"WANDB_WATCH\"] = \"all\"                  # Options: 'gradients', 'all', or 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72680c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sai\\AppData\\Local\\Temp\\ipykernel_4728\\1643594966.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8701' max='21750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8701/21750 1:18:18 < 1:57:28, 1.85 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.258100</td>\n",
       "      <td>1.248093</td>\n",
       "      <td>0.012270</td>\n",
       "      <td>0.029015</td>\n",
       "      <td>0.017247</td>\n",
       "      <td>0.420666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.253700</td>\n",
       "      <td>1.248093</td>\n",
       "      <td>0.012270</td>\n",
       "      <td>0.029015</td>\n",
       "      <td>0.017247</td>\n",
       "      <td>0.420666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.264500</td>\n",
       "      <td>1.248093</td>\n",
       "      <td>0.012270</td>\n",
       "      <td>0.029015</td>\n",
       "      <td>0.017247</td>\n",
       "      <td>0.420666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.248093</td>\n",
       "      <td>0.012270</td>\n",
       "      <td>0.029015</td>\n",
       "      <td>0.017247</td>\n",
       "      <td>0.420666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\results\\mobilebert-pii-binary3\\checkpoint-2175)... Done. 0.2s\n",
      "c:\\Users\\Sai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PII seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\results\\mobilebert-pii-binary3\\checkpoint-4350)... Done. 0.2s\n",
      "c:\\Users\\Sai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PII seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\results\\mobilebert-pii-binary3\\checkpoint-6525)... Done. 0.2s\n",
      "c:\\Users\\Sai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PII seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEvaluating model...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2171\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2169\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2170\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2171\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2172\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2176\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2625\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2622\u001b[39m     \u001b[38;5;28mself\u001b[39m.control.should_training_stop = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2624\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_epoch_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2625\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2627\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DebugOption.TPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.debug:\n\u001b[32m   2628\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[32m   2629\u001b[39m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:3079\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[39m\n\u001b[32m   3077\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_save:\n\u001b[32m   3078\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_checkpoint(model, trial)\n\u001b[32m-> \u001b[39m\u001b[32m3079\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallback_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_save\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer_callback.py:508\u001b[39m, in \u001b[36mCallbackHandler.on_save\u001b[39m\u001b[34m(self, args, state, control)\u001b[39m\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_save\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001b[32m    507\u001b[39m     control.should_save = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_event\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mon_save\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer_callback.py:519\u001b[39m, in \u001b[36mCallbackHandler.call_event\u001b[39m\u001b[34m(self, event, args, state, control, **kwargs)\u001b[39m\n\u001b[32m    517\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_event\u001b[39m(\u001b[38;5;28mself\u001b[39m, event, args, state, control, **kwargs):\n\u001b[32m    518\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m         result = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m            \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    531\u001b[39m         \u001b[38;5;66;03m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[32m    532\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\integrations\\integration_utils.py:982\u001b[39m, in \u001b[36mWandbCallback.on_save\u001b[39m\u001b[34m(self, args, state, control, **kwargs)\u001b[39m\n\u001b[32m    978\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_save\u001b[39m(\u001b[38;5;28mself\u001b[39m, args, state, control, **kwargs):\n\u001b[32m    979\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._log_model == WandbLogModel.CHECKPOINT \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._initialized \u001b[38;5;129;01mand\u001b[39;00m state.is_world_process_zero:\n\u001b[32m    980\u001b[39m         checkpoint_metadata = {\n\u001b[32m    981\u001b[39m             k: v\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mself\u001b[39m._wandb.summary).items()\n\u001b[32m    983\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, numbers.Number) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m k.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    984\u001b[39m         }\n\u001b[32m    985\u001b[39m         checkpoint_metadata[\u001b[33m\"\u001b[39m\u001b[33mmodel/num_parameters\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._wandb.config.get(\u001b[33m\"\u001b[39m\u001b[33mmodel/num_parameters\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    987\u001b[39m         ckpt_dir = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcheckpoint-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate.global_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wandb\\sdk\\wandb_summary.py:35\u001b[39m, in \u001b[36mSummaryDict.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     item = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_as_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[key]\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m     38\u001b[39m         \u001b[38;5;66;03m# this nested dict needs to be wrapped:\u001b[39;00m\n\u001b[32m     39\u001b[39m         wrapped_item = SummarySubDict()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wandb\\sdk\\wandb_summary.py:124\u001b[39m, in \u001b[36mSummary._as_dict\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_as_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_current_summary_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:387\u001b[39m, in \u001b[36m_log_to_run.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m     run_id = \u001b[38;5;28mself\u001b[39m._attach_id\n\u001b[32m    386\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m wb_logging.log_to_run(run_id):\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:1395\u001b[39m, in \u001b[36mRun._summary_get_current_summary_callback\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1392\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[32m   1394\u001b[39m get_summary_response = result.response.get_summary_response\n\u001b[32m-> \u001b[39m\u001b[32m1395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mproto_util\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdict_from_proto_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summary_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Sai\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wandb\\sdk\\lib\\proto_util.py:22\u001b[39m, in \u001b[36mdict_from_proto_list\u001b[39m\u001b[34m(obj_list)\u001b[39m\n\u001b[32m     19\u001b[39m current_level = result\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(item.nested_key) > \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     keys = \u001b[38;5;28mlist\u001b[39m(item.nested_key)\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     24\u001b[39m     keys = [item.key]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding=\"longest\")\n",
    "\n",
    "run_name = \"mobilebert-pii-binary3\"\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./results/{run_name}\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    report_to=[\"wandb\"],\n",
    "    push_to_hub=False,\n",
    "    run_name=run_name,\n",
    "    # gradient_checkpointing=True,\n",
    "    fp16=True,  # Enable mixed precision training if supported\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=binary_model,\n",
    "    args=training_args,\n",
    "    train_dataset=binary_tokenized_datasets[\"train\"],\n",
    "    eval_dataset=binary_tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training model...\")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating model...\")\n",
    "evaluation_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {evaluation_results}\")\n",
    "\n",
    "# Save model\n",
    "model_save_path = f\"./results/{run_name}/final_model\"\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.makedirs(model_save_path)\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a432290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions with the model for new text\n",
    "def mask_pii_text(text, model, tokenizer, device, id2label):\n",
    "    \"\"\"\n",
    "    Identify and mask PII entities in new text\n",
    "    \"\"\"\n",
    "    # Tokenize input with offset mapping to track character positions\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")[0]\n",
    "    \n",
    "    # Move to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=2)[0]\n",
    "    \n",
    "    # Convert token predictions to entities\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    start_idx = None\n",
    "    \n",
    "    for i, (pred, (token_start, token_end)) in enumerate(zip(predictions, offset_mapping)):\n",
    "        # Skip special tokens\n",
    "        if token_start == token_end == 0:\n",
    "            continue\n",
    "            \n",
    "        pred_label = id2label[pred.item()]\n",
    "        \n",
    "        # Check if it's beginning of entity\n",
    "        if pred_label.startswith(\"B-\"):\n",
    "            # If we were tracking a previous entity, add it to result\n",
    "            if current_entity:\n",
    "                entities.append((current_entity, start_idx, prev_end))\n",
    "            \n",
    "            # Start new entity\n",
    "            current_entity = pred_label[2:]  # Remove B- prefix\n",
    "            start_idx = token_start.item()\n",
    "            prev_end = token_end.item()\n",
    "        \n",
    "        # Check if it's inside an entity\n",
    "        elif pred_label.startswith(\"I-\") and current_entity == pred_label[2:]:\n",
    "            # Continue current entity\n",
    "            prev_end = token_end.item()\n",
    "        \n",
    "        # Not an entity or different entity\n",
    "        elif current_entity:\n",
    "            # Add previous entity to result\n",
    "            entities.append((current_entity, start_idx, prev_end))\n",
    "            current_entity = None\n",
    "    \n",
    "    # Add final entity if there is one\n",
    "    if current_entity:\n",
    "        entities.append((current_entity, start_idx, prev_end))\n",
    "    \n",
    "    # Create masked text\n",
    "    masked_text = list(text)\n",
    "    for entity_type, start, end in entities:\n",
    "        for i in range(start, end):\n",
    "            masked_text[i] = '*'\n",
    "        # Insert entity type at the beginning\n",
    "        masked_text[start:start] = f\"[{entity_type}]\"\n",
    "    \n",
    "    return ''.join(masked_text)\n",
    "\n",
    "# Example usage (uncomment to test)\n",
    "# sample_text = \"Hello, my name is John Doe and I live in New York. My phone number is 555-123-4567.\"\n",
    "# model.to(device)  # Make sure model is on the correct device\n",
    "# masked_sample = mask_pii_text(sample_text, model, tokenizer)\n",
    "# print(f\"Original: {sample_text}\")\n",
    "# print(f\"Masked: {masked_sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b99c3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your saved model\n",
    "model_path = \"./results/mobilebert-pii-masking3/final_model\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# Load the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_path,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "725e07af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hello, my name is John Doe and I live in New York. My phone number is 555-123-4567.\n",
      "Masked: Hello[S[ZI[SSN][SSN]****E]*[CRE[LITECO[LI[STATE]*[CRED[STATE[ACCOUN[LI[AC[STA[ST[[[LASTNAME]*OB]*SERNAME]*TE]*E]*OUNTNAME]***OINADDRESS]**AME]******DNUMBER]**OINADDRESS]**ADDRESS]*ITCARDNUMBER]*** my name is John Doe and I live in New York. My phone number is 555-123-4567.\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Hello, my name is John Doe and I live in New York. My phone number is 555-123-4567.\"\n",
    "masked_sample = mask_pii_text(sample_text, model, tokenizer, device, id2label)\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Masked: {masked_sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a5e139a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1: My name is John Smith and my email is johnsmith@example.com.\n",
      "Detected PII spans:\n",
      "  - My (PII): positions 0-2\n",
      "  - is John Smith (PII): positions 8-21\n",
      "  - my (PII): positions 26-28\n",
      "  - johnsmith (PII): positions 38-47\n",
      "Masked text: ** name ************* and ** email is *********@example.com.\n",
      "\n",
      "Sample 2: Please contact me at 555-123-4567 or visit me at 123 Main Street, New York, NY 10001.\n",
      "Detected PII spans:\n",
      "  - contact me (PII): positions 7-17\n",
      "  - 45 (PII): positions 29-31\n",
      "  - visit me (PII): positions 37-45\n",
      "  - Main Street, New York, (PII): positions 53-75\n",
      "Masked text: Please ********** at 555-123-**67 or ******** at 123 ********************** NY 10001.\n",
      "\n",
      "Sample 3: My social security number is 123-45-6789 and my credit card is 4111-1111-1111-1111.\n",
      "Detected PII spans:\n",
      "  - My social security (PII): positions 0-18\n",
      "  - 45 (PII): positions 33-35\n",
      "  - 678 (PII): positions 36-39\n",
      "  - my (PII): positions 45-47\n",
      "Masked text: ****************** number is 123-**-***9 and ** credit card is 4111-1111-1111-1111.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "def load_pii_model(model_path):\n",
    "    \"\"\"\n",
    "    Load the trained PII detection model and tokenizer\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model directory\n",
    "        \n",
    "    Returns:\n",
    "        model: Loaded model\n",
    "        tokenizer: Loaded tokenizer\n",
    "        id2label: Dictionary mapping from id to label\n",
    "    \"\"\"\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "    \n",
    "    # Get id2label mapping from model config\n",
    "    id2label = model.config.id2label\n",
    "    \n",
    "    # Move model to appropriate device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    return model, tokenizer, id2label\n",
    "\n",
    "def detect_pii(text, model, tokenizer, id2label, max_length=256):\n",
    "    \"\"\"\n",
    "    Detect PII in the input text using the trained model\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to analyze\n",
    "        model: Trained PII detection model\n",
    "        tokenizer: Tokenizer for the model\n",
    "        id2label: Dictionary mapping from id to label\n",
    "        max_length: Maximum sequence length for tokenization\n",
    "        \n",
    "    Returns:\n",
    "        tokens: List of tokens from the input text\n",
    "        predictions: List of predicted labels for each token\n",
    "        pii_spans: List of detected PII spans (start, end, label)\n",
    "    \"\"\"\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    # Get token offsets and move tensors to the same device as model\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\").cpu().numpy()[0]\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=2)\n",
    "    \n",
    "    # Convert predictions to labels\n",
    "    predicted_labels = [id2label[p.item()] for p in predictions[0]]\n",
    "    \n",
    "    # Get text tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    \n",
    "    # Identify PII spans (start, end, label)\n",
    "    pii_spans = []\n",
    "    current_span = None\n",
    "    \n",
    "    for i, (token, label, (start, end)) in enumerate(zip(tokens, predicted_labels, offset_mapping)):\n",
    "        # Skip special tokens like [CLS], [SEP], [PAD]\n",
    "        if token in tokenizer.special_tokens_map.values() or start == end == 0:\n",
    "            continue\n",
    "            \n",
    "        # For binary classification, we only have \"O\" and \"PII\"\n",
    "        if label == \"PII\":\n",
    "            # Start a new span if none exists\n",
    "            if current_span is None:\n",
    "                current_span = {\"start\": start, \"label\": \"PII\"}\n",
    "            # Otherwise, extend the current span\n",
    "            current_span[\"end\"] = end\n",
    "        else:  # \"O\" label or other non-PII label\n",
    "            # If we were tracking a PII span, finalize it\n",
    "            if current_span is not None:\n",
    "                pii_spans.append(current_span)\n",
    "                current_span = None\n",
    "    \n",
    "    # Don't forget to add the last span if we had one\n",
    "    if current_span is not None:\n",
    "        pii_spans.append(current_span)\n",
    "    \n",
    "    return tokens, predicted_labels, pii_spans\n",
    "\n",
    "def mask_pii_text(text, pii_spans, mask_char=\"*\"):\n",
    "    \"\"\"\n",
    "    Mask detected PII in the original text\n",
    "    \n",
    "    Args:\n",
    "        text: Original text\n",
    "        pii_spans: List of PII spans (start, end, label)\n",
    "        mask_char: Character to use for masking\n",
    "        \n",
    "    Returns:\n",
    "        masked_text: Text with PII masked\n",
    "    \"\"\"\n",
    "    # Convert text to list for easier manipulation\n",
    "    chars = list(text)\n",
    "    \n",
    "    # Apply masks\n",
    "    for span in pii_spans:\n",
    "        for i in range(span[\"start\"], span[\"end\"]):\n",
    "            chars[i] = mask_char\n",
    "    \n",
    "    # Join characters back into a string\n",
    "    masked_text = \"\".join(chars)\n",
    "    \n",
    "    return masked_text\n",
    "\n",
    "def main():\n",
    "    # Path to your saved model\n",
    "    model_path = r\"C:\\Users\\Sai\\Documents\\Neu\\Masters_Project\\PerceptionPrivacy\\pii_token_classification\\results\\mobilebert-pii-binary3\\checkpoint-8700\"\n",
    "    \n",
    "    # Load model\n",
    "    model, tokenizer, id2label = load_pii_model(model_path)\n",
    "    \n",
    "    # Example text for inference\n",
    "    sample_texts = [\n",
    "        \"My name is John Smith and my email is johnsmith@example.com.\",\n",
    "        \"Please contact me at 555-123-4567 or visit me at 123 Main Street, New York, NY 10001.\",\n",
    "        \"My social security number is 123-45-6789 and my credit card is 4111-1111-1111-1111.\"\n",
    "    ]\n",
    "    \n",
    "    # Process each text\n",
    "    for idx, text in enumerate(sample_texts):\n",
    "        print(f\"\\nSample {idx+1}: {text}\")\n",
    "        \n",
    "        # Detect PII\n",
    "        tokens, predicted_labels, pii_spans = detect_pii(text, model, tokenizer, id2label)\n",
    "        \n",
    "        # Print detected PII\n",
    "        print(\"Detected PII spans:\")\n",
    "        for span in pii_spans:\n",
    "            pii_text = text[span[\"start\"]:span[\"end\"]]\n",
    "            print(f\"  - {pii_text} ({span['label']}): positions {span['start']}-{span['end']}\")\n",
    "        \n",
    "        # Mask the text\n",
    "        masked_text = mask_pii_text(text, pii_spans)\n",
    "        print(f\"Masked text: {masked_text}\")\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
